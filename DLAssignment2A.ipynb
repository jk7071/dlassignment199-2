{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **ROLL NO 160122737199 V JITESH KUMAR**"
      ],
      "metadata": {
        "id": "vf77Oj5bd6t6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install and Import Libraries\n"
      ],
      "metadata": {
        "id": "4jDc33brKvK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow (already available in Colab but ensuring latest version)\n",
        "!pip install -q tensorflow\n",
        "\n",
        "# Import Necessary Libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import string\n",
        "import zipfile\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hcLL8vXeLPey",
        "outputId": "f9449ffb-d64e-4be0-e0ea-3822c1e8a64f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Download and Load Dakshina Dataset\n"
      ],
      "metadata": {
        "id": "I3f7k-kWLdcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Corrected Paths for Your Drive\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set correct file paths\n",
        "train_path = '/content/drive/MyDrive/dl assignment dataset/hi/lexicons/hi.translit.sampled.train.tsv'\n",
        "valid_path = '/content/drive/MyDrive/dl assignment dataset/hi/lexicons/hi.translit.sampled.dev.tsv'\n",
        "test_path  = '/content/drive/MyDrive/dl assignment dataset/hi/lexicons/hi.translit.sampled.test.tsv'\n",
        "\n",
        "# Define function to load data\n",
        "def load_data(filepath):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                inputs.append(parts[0])                     # Latin script\n",
        "                targets.append('\\t' + parts[1] + '\\n')       # Devanagari script\n",
        "    return inputs, targets\n",
        "\n",
        "# Load Train, Validation, Test sets\n",
        "input_texts, target_texts = load_data(train_path)\n",
        "input_texts_val, target_texts_val = load_data(valid_path)\n",
        "input_texts_test, target_texts_test = load_data(test_path)\n",
        "\n",
        "# Check a sample\n",
        "print(\"Sample Input (Latin):\", input_texts[0])\n",
        "print(\"Sample Target (Devanagari):\", target_texts[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0pH0qQquQbCH",
        "outputId": "57a11282-6c84-48c5-f480-b4cb88997e8c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Sample Input (Latin): अं\n",
            "Sample Target (Devanagari): \tan\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Preprocessing - Tokenization and Vectorization\n"
      ],
      "metadata": {
        "id": "_QlslDUlVr0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Preprocessing - Prepare Vocabulary, Tokenization\n",
        "\n",
        "# Build input (Latin) and target (Devanagari) character vocabularies\n",
        "input_characters = sorted(list(set(''.join(input_texts))))\n",
        "target_characters = sorted(list(set(''.join(target_texts))))\n",
        "\n",
        "# Number of unique tokens\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "# Mapping from characters to integers\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# Reverse mapping from integers to characters\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "# Max sequence lengths\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of unique input (Latin) tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output (Devanagari) tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "# Vectorize the data (prepare numpy arrays)\n",
        "import numpy as np\n",
        "\n",
        "# Create 3D zero matrices\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"int32\")\n",
        "decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length), dtype=\"int32\")\n",
        "decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        # decoder_target_data is ahead by one timestep\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "\n",
        "print(\"Preprocessing completed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "F9AWUn43Vtia",
        "outputId": "03d922ab-62db-4b9b-dddf-e50aa707eb91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique input (Latin) tokens: 63\n",
            "Number of unique output (Devanagari) tokens: 28\n",
            "Max sequence length for inputs: 19\n",
            "Max sequence length for outputs: 22\n",
            "Preprocessing completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Build Flexible Encoder-Decoder Model (LSTM by default)\n"
      ],
      "metadata": {
        "id": "VwzexwVDWCOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Build Encoder-Decoder Seq2Seq Model\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 256     # Embedding dimension\n",
        "hidden_units = 512      # Hidden state size\n",
        "cell_type = 'LSTM'      # Options: 'LSTM', 'GRU', 'SimpleRNN'\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = tf.keras.Input(shape=(None,), name='encoder_inputs')\n",
        "enc_emb = tf.keras.layers.Embedding(input_dim=num_encoder_tokens, output_dim=embedding_dim, name='encoder_embedding')(encoder_inputs)\n",
        "\n",
        "# Choose Encoder RNN Cell\n",
        "if cell_type == 'LSTM':\n",
        "    encoder_rnn = tf.keras.layers.LSTM(hidden_units, return_state=True, name='encoder_lstm')\n",
        "elif cell_type == 'GRU':\n",
        "    encoder_rnn = tf.keras.layers.GRU(hidden_units, return_state=True, name='encoder_gru')\n",
        "elif cell_type == 'SimpleRNN':\n",
        "    encoder_rnn = tf.keras.layers.SimpleRNN(hidden_units, return_state=True, name='encoder_rnn')\n",
        "\n",
        "# Encoder Outputs\n",
        "if cell_type == 'LSTM':\n",
        "    _, state_h, state_c = encoder_rnn(enc_emb)\n",
        "    encoder_states = [state_h, state_c]\n",
        "else:\n",
        "    _, state_h = encoder_rnn(enc_emb)\n",
        "    encoder_states = [state_h]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = tf.keras.Input(shape=(None,), name='decoder_inputs')\n",
        "dec_emb_layer = tf.keras.layers.Embedding(input_dim=num_decoder_tokens, output_dim=embedding_dim, name='decoder_embedding')\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Choose Decoder RNN Cell\n",
        "if cell_type == 'LSTM':\n",
        "    decoder_rnn = tf.keras.layers.LSTM(hidden_units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "elif cell_type == 'GRU':\n",
        "    decoder_rnn = tf.keras.layers.GRU(hidden_units, return_sequences=True, return_state=True, name='decoder_gru')\n",
        "elif cell_type == 'SimpleRNN':\n",
        "    decoder_rnn = tf.keras.layers.SimpleRNN(hidden_units, return_sequences=True, return_state=True, name='decoder_rnn')\n",
        "\n",
        "# Decoder Outputs\n",
        "if cell_type == 'LSTM':\n",
        "    decoder_outputs, _, _ = decoder_rnn(dec_emb, initial_state=encoder_states)\n",
        "else:\n",
        "    decoder_outputs, _ = decoder_rnn(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "# Dense layer to generate probabilities\n",
        "decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax', name='decoder_output')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the full model\n",
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "d-qStvBMWWM1",
        "outputId": "06fe307c-7cb0-4f81-91db-4938a6e09fcc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m16,128\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m7,168\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),     │  \u001b[38;5;34m1,574,912\u001b[0m │ encoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m1,574,912\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)  │     \u001b[38;5;34m14,364\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,128</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,168</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ encoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ decoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,364</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,187,484\u001b[0m (12.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,187,484</span> (12.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,187,484\u001b[0m (12.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,187,484</span> (12.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Train the Seq2Seq Model\n"
      ],
      "metadata": {
        "id": "atUwY5dtWgMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train the Seq2Seq Model\n",
        "\n",
        "# Set training hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 30  # You can increase if time permits in Colab Pro\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ytqzjvD2Wh20",
        "outputId": "9ac06862-5e77-4cd3-f45e-cf9acdb19695"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.0917 - loss: 0.9948 - val_accuracy: 0.1172 - val_loss: 1.0758\n",
            "Epoch 2/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.1649 - loss: 0.7377 - val_accuracy: 0.1548 - val_loss: 0.9091\n",
            "Epoch 3/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.2262 - loss: 0.5212 - val_accuracy: 0.2147 - val_loss: 0.5897\n",
            "Epoch 4/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.2714 - loss: 0.3557 - val_accuracy: 0.2483 - val_loss: 0.4607\n",
            "Epoch 5/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.2962 - loss: 0.2670 - val_accuracy: 0.2606 - val_loss: 0.4203\n",
            "Epoch 6/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.3081 - loss: 0.2234 - val_accuracy: 0.2740 - val_loss: 0.3712\n",
            "Epoch 7/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.3154 - loss: 0.1962 - val_accuracy: 0.2748 - val_loss: 0.3669\n",
            "Epoch 8/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.3200 - loss: 0.1754 - val_accuracy: 0.2785 - val_loss: 0.3556\n",
            "Epoch 9/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3243 - loss: 0.1626 - val_accuracy: 0.2807 - val_loss: 0.3449\n",
            "Epoch 10/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3268 - loss: 0.1512 - val_accuracy: 0.2804 - val_loss: 0.3490\n",
            "Epoch 11/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.3289 - loss: 0.1427 - val_accuracy: 0.2843 - val_loss: 0.3286\n",
            "Epoch 12/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.3304 - loss: 0.1336 - val_accuracy: 0.2836 - val_loss: 0.3364\n",
            "Epoch 13/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.3317 - loss: 0.1293 - val_accuracy: 0.2851 - val_loss: 0.3255\n",
            "Epoch 14/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3327 - loss: 0.1233 - val_accuracy: 0.2850 - val_loss: 0.3255\n",
            "Epoch 15/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3335 - loss: 0.1200 - val_accuracy: 0.2839 - val_loss: 0.3317\n",
            "Epoch 16/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.3352 - loss: 0.1145 - val_accuracy: 0.2822 - val_loss: 0.3303\n",
            "Epoch 17/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.3359 - loss: 0.1119 - val_accuracy: 0.2821 - val_loss: 0.3332\n",
            "Epoch 18/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3357 - loss: 0.1079 - val_accuracy: 0.2833 - val_loss: 0.3293\n",
            "Epoch 19/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.3372 - loss: 0.1043 - val_accuracy: 0.2840 - val_loss: 0.3288\n",
            "Epoch 20/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3366 - loss: 0.1025 - val_accuracy: 0.2831 - val_loss: 0.3333\n",
            "Epoch 21/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3381 - loss: 0.1008 - val_accuracy: 0.2807 - val_loss: 0.3358\n",
            "Epoch 22/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.3376 - loss: 0.0975 - val_accuracy: 0.2850 - val_loss: 0.3232\n",
            "Epoch 23/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3394 - loss: 0.0954 - val_accuracy: 0.2838 - val_loss: 0.3305\n",
            "Epoch 24/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.3387 - loss: 0.0951 - val_accuracy: 0.2796 - val_loss: 0.3444\n",
            "Epoch 25/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3393 - loss: 0.0930 - val_accuracy: 0.2828 - val_loss: 0.3336\n",
            "Epoch 26/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3397 - loss: 0.0917 - val_accuracy: 0.2799 - val_loss: 0.3416\n",
            "Epoch 27/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.3402 - loss: 0.0893 - val_accuracy: 0.2838 - val_loss: 0.3259\n",
            "Epoch 28/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3409 - loss: 0.0874 - val_accuracy: 0.2788 - val_loss: 0.3489\n",
            "Epoch 29/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.3404 - loss: 0.0870 - val_accuracy: 0.2795 - val_loss: 0.3416\n",
            "Epoch 30/30\n",
            "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.3408 - loss: 0.0847 - val_accuracy: 0.2801 - val_loss: 0.3431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Evaluate Model and Generate Sample Predictions\n"
      ],
      "metadata": {
        "id": "Ud_MsPs3YuUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate Model and Generate Predictions\n",
        "\n",
        "# Function to decode sequences (from predictions back to Devanagari text)\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1 with only the start character '\\t'\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_token_index['\\t']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        if cell_type == 'LSTM':\n",
        "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "            states_value = [h, c]\n",
        "        else:\n",
        "            output_tokens, h = decoder_model.predict([target_seq] + states_value)\n",
        "            states_value = [h]\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Build encoder model for inference\n",
        "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Build decoder model for inference\n",
        "decoder_state_inputs = []\n",
        "decoder_states = []\n",
        "\n",
        "decoder_inputs_single = tf.keras.Input(shape=(1,), name='decoder_input_inference')\n",
        "decoder_emb2 = dec_emb_layer(decoder_inputs_single)\n",
        "\n",
        "if cell_type == 'LSTM':\n",
        "    decoder_state_input_h = tf.keras.Input(shape=(hidden_units,))\n",
        "    decoder_state_input_c = tf.keras.Input(shape=(hidden_units,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_rnn(\n",
        "        decoder_emb2, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h2, state_c2]\n",
        "else:\n",
        "    decoder_state_input_h = tf.keras.Input(shape=(hidden_units,))\n",
        "    decoder_states_inputs = [decoder_state_input_h]\n",
        "\n",
        "    decoder_outputs2, state_h2 = decoder_rnn(\n",
        "        decoder_emb2, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h2]\n",
        "\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "decoder_model = tf.keras.Model(\n",
        "    [decoder_inputs_single] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states\n",
        ")\n",
        "\n",
        "# Test and Show 10 Sample Predictions\n",
        "for seq_index in range(10):\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "    print('Input word:', input_texts[seq_index])\n",
        "    print('Actual Devanagari:', target_texts[seq_index][1:-1])  # Remove \\t and \\n\n",
        "    print('Predicted Devanagari:', decoded_sentence.strip())\n",
        "    print('---------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xqMgtVe0Yv3o",
        "outputId": "5b28a873-37ce-4f6a-91e4-e3f4cae05dcb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Input word: अं\n",
            "Actual Devanagari: an\n",
            "Predicted Devanagari: an\n",
            "---------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Input word: अंकगणित\n",
            "Actual Devanagari: ankganit\n",
            "Predicted Devanagari: ankganit\n",
            "---------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Input word: अंकल\n",
            "Actual Devanagari: uncle\n",
            "Predicted Devanagari: uncle\n",
            "---------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Input word: अंकुर\n",
            "Actual Devanagari: ankur\n",
            "Predicted Devanagari: ankur\n",
            "---------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Input word: अंकुरण\n",
            "Actual Devanagari: ankuran\n",
            "Predicted Devanagari: ankuran\n",
            "---------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Input word: अंकुरित\n",
            "Actual Devanagari: ankurit\n",
            "Predicted Devanagari: ankurit\n",
            "---------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Input word: अंकुश\n",
            "Actual Devanagari: aankush\n",
            "Predicted Devanagari: ankush\n",
            "---------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "Input word: अंकुश\n",
            "Actual Devanagari: ankush\n",
            "Predicted Devanagari: ankush\n",
            "---------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "Input word: अंग\n",
            "Actual Devanagari: ang\n",
            "Predicted Devanagari: ang\n",
            "---------------------------------------------\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "Input word: अंग\n",
            "Actual Devanagari: anga\n",
            "Predicted Devanagari: ang\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Theoretical Answers\n"
      ],
      "metadata": {
        "id": "BAkdBBVnZeM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Total Computations\n",
        "Assumptions given:\n",
        "\n",
        "Embedding size = m\n",
        "\n",
        "Hidden units in LSTM = k\n",
        "\n",
        "Input and output sequence length = T\n",
        "\n",
        "Vocabulary size (input and output) = V\n",
        "\n",
        "Computation inside Encoder LSTM per timestep:\n",
        "\n",
        "Matrix multiplications: (input embedding + hidden state) → hidden units\n",
        "\n",
        "At each timestep:\n",
        "\n",
        "4 × (m + k) × k computations (because LSTM has 4 gates: input, forget, cell, output)\n",
        "\n",
        "Total encoder computations:\n",
        "\n",
        "𝑇\n",
        "×\n",
        "4\n",
        "×\n",
        "(\n",
        "𝑚\n",
        "+\n",
        "𝑘\n",
        ")\n",
        "×\n",
        "𝑘\n",
        "T×4×(m+k)×k\n",
        "\n",
        "Computation inside Decoder LSTM per timestep:\n",
        "\n",
        "Similar:\n",
        "\n",
        "4 × (embedding size + hidden units) × hidden units\n",
        "\n",
        "Plus Dense softmax layer output:\n",
        "\n",
        "k × V computations\n",
        "\n",
        "Total decoder computations:\n",
        "\n",
        "𝑇\n",
        "×\n",
        "(\n",
        "4\n",
        "×\n",
        "(\n",
        "𝑚\n",
        "+\n",
        "𝑘\n",
        ")\n",
        "×\n",
        "𝑘\n",
        "+\n",
        "𝑘\n",
        "×\n",
        "𝑉\n",
        ")\n",
        "T×(4×(m+k)×k+k×V)\n",
        "\n",
        "Final Total Computations =\n",
        "𝑇\n",
        "×\n",
        "[\n",
        "4\n",
        "(\n",
        "𝑚\n",
        "+\n",
        "𝑘\n",
        ")\n",
        "𝑘\n",
        "+\n",
        "4\n",
        "(\n",
        "𝑚\n",
        "+\n",
        "𝑘\n",
        ")\n",
        "𝑘\n",
        "+\n",
        "𝑘\n",
        "𝑉\n",
        "]\n",
        "T×[4(m+k)k+4(m+k)k+kV]\n",
        "Simplified:\n",
        "\n",
        "𝑇\n",
        "×\n",
        "(\n",
        "8\n",
        "(\n",
        "𝑚\n",
        "+\n",
        "𝑘\n",
        ")\n",
        "𝑘\n",
        "+\n",
        "𝑘\n",
        "𝑉\n",
        ")\n",
        "T×(8(m+k)k+kV)\n",
        "(b) Total Number of Parameters\n",
        "Input Embedding Layer (encoder):\n",
        "\n",
        "𝑉\n",
        "×\n",
        "𝑚\n",
        "V×m\n",
        "\n",
        "Input Embedding Layer (decoder):\n",
        "\n",
        "𝑉\n",
        "×\n",
        "𝑚\n",
        "V×m\n",
        "\n",
        "Encoder LSTM:\n",
        "\n",
        "4\n",
        "×\n",
        "[\n",
        "(\n",
        "𝑚\n",
        "+\n",
        "𝑘\n",
        ")\n",
        "×\n",
        "𝑘\n",
        "+\n",
        "𝑘\n",
        "]\n",
        "4×[(m+k)×k+k]\n",
        "\n",
        "Decoder LSTM:\n",
        "\n",
        "4\n",
        "×\n",
        "[\n",
        "(\n",
        "𝑚\n",
        "+\n",
        "𝑘\n",
        ")\n",
        "×\n",
        "𝑘\n",
        "+\n",
        "𝑘\n",
        "]\n",
        "4×[(m+k)×k+k]\n",
        "\n",
        "Dense Layer:\n",
        "\n",
        "𝑘\n",
        "×\n",
        "𝑉\n",
        "+\n",
        "𝑉\n",
        "k×V+V\n",
        "\n",
        "Final Total Parameters =\n",
        "2\n",
        "𝑉\n",
        "𝑚\n",
        "+\n",
        "8\n",
        "(\n",
        "𝑚\n",
        "+\n",
        "𝑘\n",
        ")\n",
        "𝑘\n",
        "+\n",
        "2\n",
        "×\n",
        "4\n",
        "𝑘\n",
        "+\n",
        "𝑘\n",
        "𝑉\n",
        "+\n",
        "𝑉\n",
        "2Vm+8(m+k)k+2×4k+kV+V\n",
        "where\n",
        "\n",
        "m = embedding size\n",
        "\n",
        "k = hidden units\n",
        "\n",
        "V = vocabulary size\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JNxyEsoQZnMy"
      }
    }
  ]
}